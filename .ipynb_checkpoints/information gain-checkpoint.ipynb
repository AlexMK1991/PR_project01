{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _entropy(dist):\n",
    "    \"\"\"\n",
    "    Entropy of class-distribution matrix\n",
    "    \n",
    "    It may be needed to change the clip to something smaler.\n",
    "    \"\"\"\n",
    "    p = dist / np.sum(dist, axis=0)\n",
    "    q = np.clip(p, 1e-15, 1)\n",
    "    return np.sum(np.sum(- p * np.log2(q), axis=0) * np.sum(dist, axis=0) / np.sum(dist))\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Information gain ratio is the ratio between information gain and\n",
    "    the entropy of the feature's\n",
    "    value distribution. The score was introduced in [Quinlan1986]_\n",
    "    to alleviate overestimation for multi-valued features. See `Wikipedia entry on gain ratio\n",
    "    <http://en.wikipedia.org/wiki/Information_gain_ratio>`_.\n",
    "    .. [Quinlan1986] J R Quinlan: Induction of Decision Trees, Machine Learning, 1986.\n",
    "    \"\"\"\n",
    "def GainRatio(examples, nan_adjustment):\n",
    "    \"\"\"\n",
    "    Returns the Information Gain-Ration, that is \\frac{Information Gain}{Intrinsic Value}\n",
    "\n",
    "    examples are all training examples, given as 2-d numpy array.\n",
    "    Assuming H is the Entropy and EX are the examples, than H(EX)shall be the same as h_class.\n",
    "    nan_adjustment shall take care of all nan Values.\n",
    "    The Information Gain is h_class-h_residual.\n",
    "    h_attribute is the Intrinsic Value.\n",
    "    If h_attribute = 0, than the Information Gain-Ration is the same as the Information Gain.\n",
    "    \"\"\"\n",
    "    h_class = _entropy(np.sum(examples, axis=1))\n",
    "    h_residual = _entropy(np.compress(np.sum(examples, axis=0), examples, axis=1))\n",
    "    h_attribute = _entropy(np.sum(examples, axis=0))\n",
    "    if h_attribute == 0:\n",
    "        h_attribute = 1\n",
    "    return nan_adjustment * (h_class - h_residual) / h_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gini(dist):\n",
    "    \"\"\"Gini index of class-distribution matrix\"\"\"\n",
    "    p = np.asarray(dist / np.sum(dist, axis=0))\n",
    "    return np.sum((1 - np.sum(p ** 2, axis=0)) *\n",
    "                  np.sum(dist, axis=0) / np.sum(dist))\n",
    "\"\"\"\n",
    "Gini impurity is the probability that two randomly chosen instances will have different\n",
    "classes. See `Wikipedia entry on Gini impurity\n",
    "<https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity>`_.\n",
    "\"\"\"\n",
    "def Gini(examples, nan_adjustment):\n",
    "    \"\"\"\n",
    "    Returns the Gini imprunity of the examples.\n",
    "    \n",
    "    examples are all training examples, given as 2-d numpy array.\n",
    "    The orientation of the numpy array is assumed ro be the same as for GainRatio.\n",
    "    nan_adjustment shall take care of all nan Values. \n",
    "    \"\"\"\n",
    "    return (_gini(np.sum(examples, axis=1)) - _gini(examples)) * nan_adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
