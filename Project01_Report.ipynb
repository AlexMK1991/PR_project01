{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Recognition - First Project Report\n",
    "\n",
    "#### The Task\n",
    "The task at hand is a supervised prediction task.\n",
    "We were given some data of a phone, sorted by time. The data included the Batery status and change between two time steps, as well as other features. Given those features we want to predict the change in batterylevel in a few timesteps. This shall be done by using the tools given by the lecture.\n",
    "\n",
    "#### Preprocessing the data\n",
    "\n",
    "This step is mainly concerned with creating representations of the data which are suitable for use in the two system components. For the prediction, we need to assign each *unique* state an integer label to index the required transition counts and transition probability matrices. This should be done without any redundance, as the dimensionality of these matrices determines memory and computing costs. As such, a base 2 representation is not suitable. \n",
    "\n",
    "For regression, however, we require the original binary activity vectors, as they are multiplied directly by the weights fitted by the linear regression. However, we cannot identify these activity vectors just by their unique state label, as even otherwise equivalent states can have different battery level deltas at the time they were observed. \n",
    "\n",
    "Overall, each state therefore recieves one (not necessarily unique) integer label to interface with the transition prediction process, but its original index is also preserved for use in the regression component. Finally, for cross-validation, each data point is linked to the day it was recorded. This allows partioning of the dataset by days, assuming user activity resets roughly at the end of each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activity vectors shape: (2445, 35)\n",
      "Targets shape: (2445,)\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "np.random.seed(123)\n",
    "dataset_df = get_table().dropna()\n",
    "mask = (dataset_df['battery_plugged'] == 0) | (dataset_df['battery_plugged'] == 1)\n",
    "dataset_df = dataset_df[mask]\n",
    "# month in 8-12, 1-3, day in 1-31\n",
    "# the following replacements keep 'monthday' chronologically sorted when hashed later\n",
    "dataset_df['month'][dataset_df['month'] == 1] = 13\n",
    "dataset_df['month'][dataset_df['month'] == 2] = 14\n",
    "dataset_df['month'][dataset_df['month'] == 3] = 15\n",
    "dataset_df['monthday'] = dataset_df['month']*100 + dataset_df['day']\n",
    "\n",
    "text = 'packages_running_'\n",
    "keep = [i for i in dataset_df.columns if text in i] + ['battery_plugged'] + ['battery_level'] + ['slot'] + ['monthday']\n",
    "dataset_df = dataset_df[keep[:49] + keep[50:54] + keep[55:]]\n",
    "dataset_df = dataset_df.dropna().T.drop_duplicates().T.reset_index()\n",
    "dataset_df['md_key'] = hash_states(dataset_df['monthday'].to_numpy()[:,None])\n",
    "dataset_df = dataset_df.drop(['monthday', 'slot'], axis=1)\n",
    "dataset_df = dataset_df.drop(['packages_running_android', 'packages_running_com.android.calculator2',\\\n",
    "                             'packages_running_com.android.keychain','packages_running_com.android.packageinstaller',\\\n",
    "                             'packages_running_com.android.providers.applications', 'packages_running_com.android.providers.downloads',\\\n",
    "                             'packages_running_com.google.android.email', 'packages_running_edu.udo.cs.ess.mobidac.target',\\\n",
    "                             'packages_running_org.openintents.filemanager', 'packages_running_stream.android'], axis=1)\n",
    "\n",
    "# get indices of dataset elements per day, so that we can use this partitioning of the data in training and validation\n",
    "num_days = dataset_df['md_key'].to_numpy().max() + 1\n",
    "# by day is a list that for each day, contains all dataset indices for that day\n",
    "by_day = [np.array(dataset_df.index[dataset_df['md_key'] == i].tolist()) for i in range(num_days)]\n",
    "# keep only days with at least 5 samples\n",
    "by_day_filtered = [item for item in by_day if len(item) > 4]\n",
    "# we can access day i by calling dataset_df.loc[by_day[i]]\n",
    "\n",
    "# in this state space, battery plugged is the last column: activity_vectors[:,-1]\n",
    "activity_vectors = dataset_df.drop(['index', 'battery_level', 'md_key'], axis=1).to_numpy()\n",
    "targets = dataset_df['battery_level'].to_numpy()\n",
    "print('Activity vectors shape:', activity_vectors.shape)\n",
    "print('Targets shape:', targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State space transforms\n",
    "\n",
    "As the State space contains quite many different states and computing the makrov chain depends quadratically on those, we desire a low number of unique states. As this comes at a cost we want to minimise the error induced by a reduction of the states. Therefore we compared different pruning methods and decided on one reduction method.  \n",
    "We firstly threw out all features which were constant through out all examples, as they don't contain any information. Considering this state space we computed the Information gain ratio and the Gini gain and kept the best $\\mathcal{N}$ features, where $\\mathcal{N}\\in[1,\\textsf{#FEATURES}]$. In consequence, larger portions of the state space can collapse to the same representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique states without pruning: 637\n",
      "Number of unique states: 291\n"
     ]
    }
   ],
   "source": [
    "targets_binary_1 = (np.sign(targets)==1).astype(int)[None]\n",
    "\n",
    "state_space_transform_mode = 'Gini'\n",
    "states = state_space_transform(activity_vectors, targets=targets_binary_1, mode=state_space_transform_mode)\n",
    "keep_best = 12\n",
    "\n",
    "states = np.concatenate([states[:,:keep_best], states[:, -1, None]], axis=1)\n",
    "out_labels_full = hash_states(activity_vectors)\n",
    "num_unique_states_full = out_labels_full.max()+1\n",
    "print('Number of unique states without pruning:', num_unique_states_full)\n",
    "\n",
    "out_labels_pruned = hash_states(states)\n",
    "num_unique_states_pruned = out_labels_pruned.max()+1\n",
    "print('Number of unique states:', num_unique_states_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction:\n",
    "\n",
    "The input to the prediction system should be sequences of observed transitions between states. These are readily represented by lists of lists containing state labels. Subsequent state labels $i$ and $j$ within the same (lower-level) list represent an oberved transition from state $i$ to state $j$, and a matrix $N =(n)_{j,i}$ counting the state transitions is incremented accordingly. Aggregated over all lists,  $N$ counts all observed transitions in the data set, while also respecting reset of user behaviour.\n",
    "\n",
    "##### MLE Prediction\n",
    "\n",
    "For the MLE predictor, we simply normalize $N$ to the column-stochastic matrix $\\theta_{MLE}$, which means that the entry \n",
    " $\\theta_{MLE}$ at index $(j,i)$ represents the frequency of state transitions to the state with the label $j$ given that the current state has the label $i$. \n",
    "\n",
    "##### MAP Prediction\n",
    "\n",
    "The MAP prediction process is much more involved and requires approximation of the expectation $E[\\theta,\\mathcal{D}]$ using many randomly generated transition probability matrices $\\theta$. These are averaged using a weighting scheme determined by the extent to which they can explain the observed transitions. Overall, individual weights are expected to become extremely small due to the high number of matrix entries. This means that some exp-log tricks with a subsequent shift of the sample weights in logspace are required to achieve runtimes compatible with exhaustive cross-validation. Thus, our code does not look very familiar, but in principle it is based purely on the example Sebastian showed in a previous exercise. \n",
    "\n",
    "To justify the usage of MAP prediction given the comparitively high computational costs, regard the following dummy example of a Markov Chain with transition probabilities described by\n",
    "\n",
    "$\\begin{pmatrix}\n",
    " \\frac{1}{3} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{6} \\\\\n",
    " \\frac{1}{6} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{3} \\\\\n",
    " \\frac{1}{3} & \\frac{1}{4} & \\frac{2}{5} & \\frac{1}{6} \\\\\n",
    " \\frac{1}{6} & \\frac{1}{4} & \\frac{1}{5} & \\frac{1}{3}\n",
    "\\end{pmatrix}$\n",
    "\n",
    "for which we simulate a trajectory containing a small number of state transitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictor input:\n",
      "[0 2 1 0 2 2 2 3 3 1 1 1 2 2 0]\n",
      "[1 2 0 0 2 2 2 3 3 2 2 1 1 0 0]\n",
      "Code block execution took 0.0014426708221435547 seconds.\n",
      "Ground Truth:\n",
      " [[0.33333333 0.25       0.2        0.16666667]\n",
      " [0.16666667 0.25       0.2        0.33333333]\n",
      " [0.33333333 0.25       0.4        0.16666667]\n",
      " [0.16666667 0.25       0.2        0.33333333]] \n",
      "\n",
      "MLE:\n",
      " [[0.4        0.28571429 0.16666667 0.        ]\n",
      " [0.         0.42857143 0.16666667 0.25      ]\n",
      " [0.6        0.28571429 0.5        0.25      ]\n",
      " [0.         0.         0.16666667 0.5       ]] \n",
      "\n",
      "MAP:\n",
      " [[0.34736398 0.23204555 0.21750678 0.15624641]\n",
      " [0.11083901 0.32839146 0.2199273  0.27525666]\n",
      " [0.41380775 0.31793639 0.3834216  0.23226108]\n",
      " [0.1279891  0.12162656 0.17914411 0.33623573]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a minimal example of the MLE/MAP prediction process\n",
    "# first, we define some ground truth transition matrix for a dummy Markov Chain\n",
    "gt = np.array([[1./3., 1./4., 1./5., 1./6.], [1./6., 1./4., 1./5, 1./3.], [1./3., 1./4., 2./5, 1./6.], [1./6., 1./4., 1./5., 1./3.]])\n",
    "# start in some state, here it is 0\n",
    "traj = [int(0)]\n",
    "# get only a small number of sample transitions within this Markov Chain, add them to the 'trajectory'\n",
    "nn = 29\n",
    "#simulate the process\n",
    "for i in range(nn):\n",
    "    traj.append(np.random.choice([0, 1, 2, 3], p=gt[:,traj[-1]]).astype(int))\n",
    "traj = np.array(traj).astype(int)\n",
    "# here we simply split the sequence traj into traj[:15], traj[15:] to emphasize that the input is a list of sequences\n",
    "predictor_input = [traj[:15], traj[15:]]\n",
    "print('Predictor input:')\n",
    "for sequence_observed in predictor_input:\n",
    "    print(sequence_observed)\n",
    "start = time.time()\n",
    "mle_P = fit_predictor(predictor_input, num_unique_states=traj.max()+1, mode='MLE')\n",
    "map_P = fit_predictor(predictor_input, num_unique_states=traj.max()+1, mode='MAP')\n",
    "print('Code block execution took', time.time()-start, 'seconds.')\n",
    "print('Ground Truth:\\n', gt, '\\n')\n",
    "print('MLE:\\n', mle_P, '\\n')\n",
    "print('MAP:\\n', map_P, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: When the space of state transitions is undersampled, the MLE result is good at reproducing the observed data, but might be very bad at generalizing to predicting unseen state transitions. For a relatively low amount of samples, we expect MAP to achieve better prediction results.\n",
    "\n",
    "#### Prediction results\n",
    "\n",
    "Keeping this in mind, we move to apply these methods on the real dataset. First, we use the previous partition of the data into days to perform leave-one-(day)-out cross-validation. Regard the results of the MLE predictor averaged over all validation splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/MLE_predictor_bitflip.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "It is evident that the performance, measured in percentage bit-flips between the ground truth state and the predicted state, differs vastly between training and validation sets. While it is acceptable on the training set, the validation set loss clearly shows signs of model overfitting, which is what we expect from MLE prediction since the observed number of state transitions is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/MAP_prediction_bitflip.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "MAP prediction, however, generalizes very well from the training to the validation sets on average. Visualizing these errors based on the validation split demonstrates that they are generally much better than a random prediction over all of the train-valid data partitions.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/MAP_prediction_bitflip_by_split.png\" heigth=\"1200\">\n",
    "</div>\n",
    "\n",
    "Note that we can also measure these errors in terms of the BCE, which shows similar, slightly smoother, albeit more difficult to intepret results. Overall, both terms are mostly interchangeable.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/MAP_prediction_bce_by_split.png\" width=\"1200\">\n",
    "</div>\n",
    "\n",
    "Therefore, we constrain the evaluation of the state transition prediction to the bit-flip losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Output Transforms\n",
    "\n",
    "Above the prediction output is compared to the activity vectors corresponding to the ground truth state. This is not possible without further consideration, because the output of the prediction is just a probability distribution over the set of unique states. Bluntly put, the activity vector has $\\textsf{#FEATURES}$ entries, whereas the prediction output has $\\textsf{#UNIQUE STATES}$ entries.\n",
    "\n",
    "Naively, the most straightforward option would be to take the argmax of the state distribution and then look up the corresponding activity vector. However, this discards alot of useful information, especially in the face of very uncertain state transitions. Therefore, we also experimented with converting the output distribution of the predictor to a set of individual state-space feature activation distributions via a weighted sum over all unique state vectors. The result can also be rounded in each component, yielding a nearest-neighbor fit that can even produce unobserved states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "We implement the regression components in the closed form solutions given in the lecture, which is possible because we took care to remove redundancies in the state space in the preprocessing, guaranteeing invertible hat matrices. Explicitly, the formulas used were as follows:\n",
    "\n",
    "##### MLE\n",
    "\n",
    "$w_{\\theta_{MLE}} = (X X^T)^{-1} X y$\n",
    "\n",
    "##### MAP\n",
    "\n",
    "$w_{\\theta_{MAP}} = (X X^T + \\sigma Id)^{-1} X y$\n",
    "\n",
    "with $X$ the data matrix constructed by concatentating the state vectors with a bias vector, the targets $y$ and $\\sigma = \\operatorname{var}(y)$. The regressor output is then given by\n",
    "\n",
    "$\\hat{y} = X^T w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression results\n",
    "\n",
    "At first glance, the results of the MLE regressor, measured as the absolute error between ground truth battery delta and the regressor output, seems quite acceptable.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/MLE_regression_overview.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "The overall result of the MAP regression does not point out any obvious advantages here.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/MAP_regression_overview.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "However, splitting the sample errors by battery plugged state reveals overfitting in the situations where the phone is plugged in for charging, as the number of samples is not large enough for these states. This is 'hidden' in the overall error, also because the number of samples is small.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/MLE_regression_by_battery.png\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "\n",
    "As expected, MAP does better for these cases, and generalizes well. When comparing these two plots, note the scale of the y-axis.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/MAP_regression_by_battery.png\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target Space Transforms\n",
    "\n",
    "As the regression only yields a linear function, adjusting the target space to be more suitable for a linear fit might be beneficial. To test this, we mapped the negative battery deltas to the square root of their absolute values. After the regressor is fit, its estimates can be converted by applying the inverse function to $\\hat{y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the overall system\n",
    "\n",
    "At this point, we have many different potential configurations of regressors and predictors, starting with the basic subdivision into MLE or MAP approaches, but also encompassing more subtle choices like state or target space transform modes, how much cropping is done based on the Gini or Informations gains. Evaluation of every combination of hyperparameters is not straightforward, because each trial requires full calculation of the cross-validation scheme. This could amount to, for example, around 110 calculations of the MAP prediction matrix per trial, with hundreds of trials necessary to cover the whole hyperparameter space.\n",
    "\n",
    "Using a functionality provided by [weights and biases](https://wandb.ai/), we complete this grid search in a distributed manner. Based on the (somewhat shady) assumption that predictor and regressor are independent subsystems, we evaluate all variations of the config shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_defaults = dict(\n",
    "    state_transform_mode = 'Id', #'Gain', 'Gini'\n",
    "    keep_best = 16, #1-34\n",
    "    predictor_mode = 'MLE', #'MAP'\n",
    "    prediction_transform = 'argmax', #'activity_dist', 'argmax', 'nearest_neighbor'\n",
    "    regressor_mode = 'MLE', #'MAP'\n",
    "    charge_transform_mode = 'Id',\n",
    "    discharge_transform_mode = 'Id', #'Sqrt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results, remaining problems\n",
    "\n",
    "To evaluate the full system, we arbitrarily select one split into training and validation data as an example. This is justified considering that our components previously showed consistant performance over all training and validation splits. Predictor and regressor are now fitted using exactly the same training data. The predictor generates results over $1$ to $4$ timesteps, which are converted to activity distributions and fed directly into the regressor that was previously fitted to the training data. The absolute error of the battery level changes is aggregated over all timesteps. \n",
    "\n",
    "First, we can observe that our system performance is bottle-necked by the prediction accuracy.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/Full_system_bitflips_vs_time.png\" width=\"1200\">\n",
    "</div>\n",
    "\n",
    "The percentage of bit-flips increases with increasing prediction time horizon, converging to values only slightly better than a random prediction. This reflects the large uncertainty inherent to each prediction step. However, the regressor performs relatively well despite this, indicating that the uncertainty might have been successfully concentrated in less relevant components of the state space.\n",
    "\n",
    "<div>\n",
    "<img src=\"img/Full_system_L1_vs_time.png\" width=\"1200\">\n",
    "</div>\n",
    "\n",
    "It should be noted that the number of training examples is so low for states in which the battery is being charged that the predictor, over time, basically refused to consider that the state might change from a discharging to a charging one. Therefore, we can only derive predictions for time horizons until which we expect the user to not have access to a charger.\n",
    "\n",
    "However, for these scenarios, we can predict up to $2$ hours into the future, with total absolute errors of around $4$. Compared to the range of these values in the data, $[-10, 30]$ per interval, this total over up to $4$ intervals seems acceptable. Using both components in the MAP configuration, the system generalizes very well from the training to the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patternrec_kernel",
   "language": "python",
   "name": "patternrec_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
